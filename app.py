import os
import shutil
import tempfile
from typing import List

import streamlit as st
from dotenv import load_dotenv

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma  # æ–°åŒ…åï¼ŒLangChain 1.0+ æ¨è

from langchain_openai import ChatOpenAI
from langchain_deepseek import ChatDeepSeek

from langchain_community.document_loaders import PyPDFLoader, TextLoader
import docx2txt


# ============ å…¨å±€å¸¸é‡é…ç½® ============
CHROMA_PERSIST_DIRECTORY = "./chroma_db"
EMB_MODEL_NAME = "BAAI/bge-large-zh-v1.5"  # å‡çº§ä¸ºæ›´å¼ºçš„ä¸­æ–‡æ¨¡å‹


def init_page() -> None:
    st.set_page_config(page_title="ä¸­æ–‡ RAG çŸ¥è¯†åº“åŠ©æ‰‹", page_icon="ğŸ“š", layout="wide")

    st.markdown(
        """
        <style>
        .main-header {font-size: 2.0rem; font-weight: 700; margin-bottom: 0.3rem;}
        .sub-header {font-size: 0.95rem; color: #666666; margin-bottom: 1.2rem;}
        </style>
        """,
        unsafe_allow_html=True,
    )

    st.markdown('<div class="main-header">ğŸ“š ä¸­æ–‡ RAG çŸ¥è¯†åº“åŠ©æ‰‹ï¼ˆQwen / DeepSeek + Chromaï¼‰</div>', unsafe_allow_html=True)
    st.markdown(
        '<div class="sub-header">ä¸Šä¼  PDF / DOCX / TXT æ–‡æ¡£ï¼Œæ„å»ºæœ¬åœ°å‘é‡çŸ¥è¯†åº“ï¼Œä½¿ç”¨äº‘ç«¯å¤§æ¨¡å‹è¿›è¡Œé«˜è´¨é‡é—®ç­”ã€‚</div>',
        unsafe_allow_html=True,
    )


@st.cache_resource(show_spinner="æ­£åœ¨åŠ è½½ä¸­æ–‡åµŒå…¥æ¨¡å‹ï¼ˆä»…é¦–æ¬¡è¾ƒæ…¢ï¼‰...")
def get_embeddings() -> HuggingFaceEmbeddings:
    os.environ.setdefault("HF_HUB_DISABLE_TELEMETRY", "1")
    return HuggingFaceEmbeddings(
        model_name=EMB_MODEL_NAME,
        encode_kwargs={"normalize_embeddings": True},
    )


def get_vectorstore() -> Chroma:
    """ç»Ÿä¸€è·å–ï¼ˆæˆ–åˆ›å»ºï¼‰å‘é‡åº“å®ä¾‹ï¼Œæ”¯æŒå¢é‡æ·»åŠ ã€‚"""
    embeddings = get_embeddings()
    return Chroma(
        persist_directory=CHROMA_PERSIST_DIRECTORY,
        embedding_function=embeddings,
    )


def ingest_docs(uploaded_files: List) -> int:
    """åŠ è½½ã€åˆ‡åˆ†å¹¶å¢é‡å†™å…¥å‘é‡åº“ï¼Œè¿”å›æœ¬æ¬¡æ–°å¢çš„ chunk æ•°é‡ã€‚"""
    if not uploaded_files:
        return 0

    raw_docs: List[Document] = []
    temp_dirs = []  # æ”¶é›†ä¸´æ—¶ç›®å½•ï¼Œä¾¿äºæ¸…ç†

    for f in uploaded_files:
        suffix = os.path.splitext(f.name)[1].lower()
        temp_dir = tempfile.mkdtemp(prefix="rag_upload_")
        temp_dirs.append(temp_dir)
        file_path = os.path.join(temp_dir, f.name)

        with open(file_path, "wb") as out_f:
            out_f.write(f.getbuffer())

        try:
            if suffix == ".pdf":
                loader = PyPDFLoader(file_path)
                docs = loader.load()
            elif suffix in [".txt", ".md"]:
                loader = TextLoader(file_path, encoding="utf-8")
                docs = loader.load()
            elif suffix in [".docx", ".doc"]:
                text = docx2txt.process(file_path)
                if text.strip():
                    docs = [Document(page_content=text, metadata={"source": f.name})]
                else:
                    docs = []
            else:
                st.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š{suffix}ï¼Œå·²è·³è¿‡ {f.name}")
                continue

            for d in docs:
                d.metadata.setdefault("source", f.name)
            raw_docs.extend(docs)
        except Exception as e:
            st.error(f"åŠ è½½ {f.name} æ—¶å‡ºé”™ï¼š{e}")

    if not raw_docs:
        return 0

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""],
    )
    split_docs = splitter.split_documents(raw_docs)

    # å¢é‡æ·»åŠ ï¼šä½¿ç”¨ from_documents ä¼šè‡ªåŠ¨å¤„ç†ç°æœ‰é›†åˆ
    vectorstore = get_vectorstore()
    added_ids = vectorstore.add_documents(split_docs)  # è¿”å›æ–°å¢çš„ ID

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    for td in temp_dirs:
        shutil.rmtree(td, ignore_errors=True)

    return len(added_ids)


def get_retriever():
    """è·å–æ£€ç´¢å™¨ï¼ˆå¦‚æœåº“ä¸ºç©ºè¿”å› Noneï¼‰ã€‚"""
    vectorstore = get_vectorstore()
    if vectorstore._collection.count() == 0:
        return None
    return vectorstore.as_retriever(search_kwargs={"k": 4})


def get_llm(provider: str, model_name: str):
    load_dotenv(override=False)

    if provider == "Qwen (é€šä¹‰åƒé—®)":
        api_key = os.getenv("DASHSCOPE_API_KEY", "").strip()
        if not api_key:
            st.error("æœªæ£€æµ‹åˆ° DASHSCOPE_API_KEYï¼Œè¯·åœ¨ .env ä¸­é…ç½®ã€‚")
            return None
        return ChatOpenAI(
            model=model_name,
            api_key=api_key,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            streaming=True,
        )

    elif provider == "DeepSeek":
        api_key = os.getenv("DEEPSEEK_API_KEY", "").strip()
        if not api_key:
            st.error("æœªæ£€æµ‹åˆ° DEEPSEEK_API_KEYï¼Œè¯·åœ¨ .env ä¸­é…ç½®ã€‚")
            return None
        return ChatDeepSeek(
            model=model_name,
            api_key=api_key,
            streaming=True,
        )

    st.error("æœªçŸ¥çš„ LLM æä¾›æ–¹ã€‚")
    return None


def build_rag_chain(retriever, llm):
    system_prompt = (
        "ä½ æ˜¯ä¸€åä¸“ä¸šçš„ä¸­æ–‡ AI åŠ©æ‰‹ï¼ŒåŸºäºæä¾›çš„çŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚\n"
        "è¦æ±‚ï¼š\n"
        "1. å›ç­”å¿…é¡»ä½¿ç”¨åœ°é“ã€æµç•…çš„ä¸­æ–‡ã€‚\n"
        "2. ä¸¥æ ¼ä¾æ® context ä¸­çš„ä¿¡æ¯æ¨ç†ï¼Œç¦æ­¢èƒ¡ç¼–ä¹±é€ ã€‚\n"
        "3. å¦‚æœ context ä¸è¶³ä»¥å›ç­”ï¼Œè¯·æ˜ç¡®è¯´â€œçŸ¥è¯†åº“ä¸­æš‚æ— è¶³å¤Ÿä¿¡æ¯â€ï¼Œå¹¶ç»™å‡ºåˆç†æ¨æµ‹ã€‚\n"
        "4. å›ç­”ç»“æ„æ¸…æ™°ï¼Œå¯ä½¿ç”¨åˆ†ç‚¹ã€åˆ—è¡¨ç­‰æ ¼å¼ã€‚"
    )

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "ç›¸å…³çŸ¥è¯†åº“å†…å®¹ï¼š\n{context}\n\né—®é¢˜ï¼š{question}"),
        ]
    )

    def format_docs(docs: List[Document]) -> str:
        return "\n\n---\n\n".join(
            f"[æ¥æºï¼š{d.metadata.get('source', 'æœªçŸ¥')}] {d.page_content}" for d in docs
        )

    rag_chain = (
        RunnableParallel(
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
        )
        | prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain


def display_sources(query: str, retriever) -> None:
    if retriever is None:
        return

    docs = retriever.invoke(query)
    if not docs:
        st.info("æœ¬æ¬¡æé—®æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ç‰‡æ®µã€‚")
        return

    st.markdown("---")
    st.subheader("ğŸ“ æ£€ç´¢åˆ°çš„å‚è€ƒæ–‡æ¡£ç‰‡æ®µ")

    for i, doc in enumerate(docs, 1):
        source = doc.metadata.get("source", f"ç‰‡æ®µ {i}")
        with st.expander(f"ğŸ“„ {source}ï¼ˆç‰‡æ®µ {i}ï¼‰", expanded=False):
            st.markdown(doc.page_content)


def sidebar_controls() -> tuple[str, str, List]:
    with st.sidebar:
        st.header("âš™ï¸ é…ç½®")

        provider = st.selectbox("é€‰æ‹©å¤§æ¨¡å‹æä¾›æ–¹", ["Qwen (é€šä¹‰åƒé—®)", "DeepSeek"], index=0)

        default_model = "qwen-plus" if provider == "Qwen (é€šä¹‰åƒé—®)" else "deepseek-chat"
        model_name = st.text_input("æ¨¡å‹åç§°", value=default_model)

        st.markdown("---")
        st.subheader("ğŸ“ ä¸Šä¼ æ–‡æ¡£ï¼ˆå¢é‡æ„å»ºçŸ¥è¯†åº“ï¼‰")
        uploaded_files = st.file_uploader(
            "æ”¯æŒ PDF / DOCX / TXT / MDï¼Œå¤šæ–‡ä»¶ä¸Šä¼ ",
            accept_multiple_files=True,
            type=["pdf", "txt", "docx", "doc", "md"],
        )

        if st.button("ğŸš€ å¼€å§‹ç´¢å¼•æ–‡æ¡£", use_container_width=True):
            if not uploaded_files:
                st.warning("è¯·å…ˆä¸Šä¼ æ–‡æ¡£ã€‚")
            else:
                with st.spinner("æ­£åœ¨å¤„ç†å¹¶å‘é‡åŒ–æ–‡æ¡£..."):
                    added = ingest_docs(uploaded_files)
                if added > 0:
                    st.success(f"æœ¬æ¬¡æˆåŠŸæ–°å¢ {added} ä¸ªæ–‡æ¡£ç‰‡æ®µã€‚")
                else:
                    st.warning("æœªæ–°å¢ä»»ä½•ç‰‡æ®µï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹ã€‚")

        st.markdown("---")
        st.caption(
            "âœ… å‘é‡åº“ï¼šChroma æœ¬åœ°æŒä¹…åŒ–\n"
            "âœ… åµŒå…¥æ¨¡å‹ï¼šBAAI/bge-large-zh-v1.5ï¼ˆæœ¬åœ°æ¨ç†ï¼Œæ— éœ€ APIï¼‰"
        )

    return provider, model_name, uploaded_files


def render_chat_area(provider: str, model_name: str) -> None:
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    user_query = st.chat_input("è¾“å…¥é—®é¢˜ï¼ŒæŒ‰å›è½¦å‘é€...")
    if not user_query:
        return

    retriever = get_retriever()
    if retriever is None:
        st.warning("çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ å¹¶ç´¢å¼•æ–‡æ¡£ã€‚")
        return

    st.session_state.messages.append({"role": "user", "content": user_query})
    with st.chat_message("user"):
        st.markdown(user_query)

    llm = get_llm(provider, model_name)
    if llm is None:
        return

    rag_chain = build_rag_chain(retriever, llm)

    with st.chat_message("assistant"):
        response = st.write_stream(rag_chain.stream(user_query))
        st.session_state.messages.append({"role": "assistant", "content": response})

    display_sources(user_query, retriever)


def main() -> None:
    load_dotenv(override=False)
    init_page()
    provider, model_name, _ = sidebar_controls()
    render_chat_area(provider, model_name)


if __name__ == "__main__":
    main()
