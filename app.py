import os
import tempfile
import uuid
import re
import numpy as np
from typing import List, Any, Tuple
import streamlit as st
from dotenv import load_dotenv

# v1.0 æ ¸å¿ƒå¯¼å…¥
from langchain_core.documents import Document
from langchain_core.stores import InMemoryStore
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.retrievers import BaseRetriever
from langchain_core.messages import HumanMessage, AIMessage

# v1.0 ç»„ä»¶
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.document_compressors import FlashrankRerank


# ä¼˜åŒ–çš„æ··åˆæ£€ç´¢å™¨
class HybridParentRetriever(BaseRetriever):
    """v1.0 æ··åˆæ£€ç´¢å™¨ï¼šå‘é‡æ£€ç´¢(å­æ–‡æ¡£) + BM25(çˆ¶æ–‡æ¡£) + çˆ¶æ–‡æ¡£æ˜ å°„"""

    vectorstore: Chroma
    docstore: InMemoryStore#æ–‡æ¡£å­˜å‚¨
    bm25_docs: List[Document]
    k1: int = 6  # å‘é‡æ£€ç´¢å­æ–‡æ¡£æ•°é‡
    k2: int = 4  # BM25 æ£€ç´¢çˆ¶æ–‡æ¡£æ•°é‡

    def _get_relevant_documents(self, query: str, *, run_manager: Any = None) -> List[Document]:
        return self.invoke(query, config=run_manager.config if run_manager else None)

    def invoke(self, query: str, config: Any = None, **kwargs: Any) -> List[Document]:
        """æ ¸å¿ƒæ£€ç´¢é€»è¾‘"""
        # 1. å‘é‡æ£€ç´¢å­æ–‡æ¡£
        child_retriever = self.vectorstore.as_retriever(search_kwargs={"k": self.k1})
        child_docs = child_retriever.invoke(query, config=config)

        # 2. BM25 æ£€ç´¢çˆ¶æ–‡æ¡£
        kw_docs = bm25_search_docs(query, self.bm25_docs, top_k=self.k2)

        # 3. å­æ–‡æ¡£ -> çˆ¶æ–‡æ¡£æ˜ å°„ (åŸºäº source å’Œç›¸ä¼¼æ€§)
        vec_parent_docs = []
        for child_doc in child_docs:
            parent_candidates = [
                doc for doc in self.bm25_docs
                if doc.metadata.get("source") == child_doc.metadata.get("source")
            ]
            if parent_candidates:
                vec_parent_docs.append(parent_candidates[0])  # å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„çˆ¶æ–‡æ¡£

        # 4. èåˆå»é‡
        all_docs = vec_parent_docs + kw_docs
        seen_ids = set()# ç”¨äºå»é‡
        unique_docs = []# å»é‡åçš„å­æ–‡æ¡£

        for doc in all_docs:
            doc_id = doc.metadata.get("doc_id") or hash(doc.page_content[:100])
            if doc_id not in seen_ids:
                seen_ids.add(doc_id)
                unique_docs.append(doc)

        return unique_docs[:self.k1 + self.k2]


# ============ å…¨å±€é…ç½® ============
CHROMA_PERSIST_DIRECTORY = "./chroma_db_parent"
EMB_MODEL_NAME = "BAAI/bge-small-zh-v1.5"
MAX_HISTORY_LENGTH = 10


# ============ åˆå§‹åŒ–é¡µé¢ ============
def init_page() -> None:
    st.set_page_config(page_title="Pro RAG (LangChain v1.0)", page_icon="ğŸš€", layout="wide")

    st.markdown("""
        <style>
        .main-header {font-size: 1.8rem; font-weight: 700; color: #1f77b4;}
        .stChatInput {position: fixed; bottom: 20px;}
        </style>
    """, unsafe_allow_html=True)

    st.markdown('<div class="main-header">ğŸš€ Pro RAG v1.0: Hybrid Search + FlashRank</div>',
                unsafe_allow_html=True)

    # åˆå§‹åŒ– Session State
    if "docstore" not in st.session_state:
        st.session_state["docstore"] = InMemoryStore()
    if "bm25" not in st.session_state:
        st.session_state["bm25"] = None
    if "bm25_docs" not in st.session_state:
        st.session_state["bm25_docs"] = []
    if "messages" not in st.session_state:
        st.session_state["messages"] = []
    if "chat_history" not in st.session_state:
        st.session_state["chat_history"] = []


# ============ æ ¸å¿ƒç»„ä»¶ ============
@st.cache_resource
def get_embeddings() -> HuggingFaceEmbeddings:
    os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"
    return HuggingFaceEmbeddings(
        model_name=EMB_MODEL_NAME,
        encode_kwargs={"normalize_embeddings": True},
        model_kwargs={"device": "cpu"}
    )


def get_vectorstore() -> Chroma:
    embedding = get_embeddings()

    vectorstore = Chroma(
        collection_name="split_parents",
        persist_directory=CHROMA_PERSIST_DIRECTORY,
        embedding_function=embedding
    )

    # å¦‚æœ docstore ä¸ºç©ºä½† chroma æœ‰æ•°æ®ï¼Œé‡ç½®
    if len(list(st.session_state["docstore"].yield_keys())) == 0 and vectorstore._collection.count() > 0:
        st.warning("âš ï¸ DocStore ä¸ Chroma ä¸ä¸€è‡´ï¼Œæ­£åœ¨é‡ç½®...")
        vectorstore.reset_collection()

    return vectorstore


def get_hybrid_retriever() -> HybridParentRetriever:
    """v1.0 æ··åˆæ£€ç´¢å™¨æ„å»º"""
    if not st.session_state["bm25_docs"]:
        return None

    return HybridParentRetriever(
        vectorstore=get_vectorstore(),
        docstore=st.session_state["docstore"],
        bm25_docs=st.session_state["bm25_docs"]
    )


# ============ BM25 å®ç° ============
def _tokenize_zh(text: str) -> List[str]:
    return re.findall(r"[\u4e00-\u9fff]+|[a-zA-Z0-9]+", text)


def rebuild_bm25(docs: List[Document]):
    """é‡å»º BM25 ç´¢å¼•"""
    if not docs:
        st.session_state["bm25"] = None
        return

    try:
        from rank_bm25 import BM25Okapi
        tokenized_corpus = [_tokenize_zh(d.page_content) for d in docs]
        st.session_state["bm25"] = BM25Okapi(tokenized_corpus)
        st.session_state["bm25_docs"] = docs
    except ImportError:
        st.error("è¯·å®‰è£…: pip install rank_bm25")
        st.stop()


def bm25_search_docs(query: str, docs: List[Document], top_k: int = 4) -> List[Document]:
    """BM25 æ£€ç´¢"""
    bm25 = st.session_state.get("bm25")
    if not bm25 or not docs:
        return docs[:top_k]  # é™çº§åˆ°ç®€å•åˆ‡ç‰‡

    tokenized_query = _tokenize_zh(query)
    scores = bm25.get_scores(tokenized_query)
    top_indices = np.argsort(scores)[-top_k:][::-1]

    results = [docs[i] for i in top_indices if scores[i] > 0]
    return results[:top_k]


# ============ æ–‡æ¡£å¤„ç† ============
def ingest_files(uploaded_files):
    """æ–‡æ¡£ç´¢å¼• - v1.0 ä¿®å¤ç‰ˆ"""
    if not uploaded_files:
        return

    raw_docs = []
    with st.status("ğŸ“¥ æ­£åœ¨å¤„ç†æ–‡æ¡£...", expanded=True) as status:
        for file in uploaded_files:
            status.write(f"è§£ææ–‡ä»¶: {file.name}")
            suffix = os.path.splitext(file.name)[1].lower()

            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
                tmp.write(file.getbuffer())
                tmp_path = tmp.name

            try:
                if suffix == ".pdf":
                    loader = PyPDFLoader(tmp_path)
                    raw_docs.extend(loader.load())
                elif suffix in [".txt", ".md"]:
                    loader = TextLoader(tmp_path, encoding="utf-8")
                    raw_docs.extend(loader.load())
                elif suffix in [".docx", ".doc"]:
                    try:
                        import docx2txt
                        text = docx2txt.process(tmp_path)
                        raw_docs.append(Document(
                            page_content=text,
                            metadata={"source": file.name}
                        ))
                    except ImportError:
                        st.warning("docx2txt æœªå®‰è£…ï¼Œè·³è¿‡ DOCX æ–‡ä»¶")
                        continue
            except Exception as e:
                st.error(f"âŒ è§£æå¤±è´¥ {file.name}: {e}")
            finally:
                try:
                    os.remove(tmp_path)
                except:
                    pass

        if not raw_docs:
            st.error("âŒ æœªè§£æåˆ°ä»»ä½•æ–‡æ¡£å†…å®¹")
            return

        status.write("ğŸ”¨ æ„å»ºçŸ¥è¯†åº“ç´¢å¼•...")

        # 1. çˆ¶æ–‡æ¡£åˆ†å‰² (å¤§å—)
        parent_splitter = RecursiveCharacterTextSplitter(
            chunk_size=2000, chunk_overlap=200
        )
        parent_docs = parent_splitter.split_documents(raw_docs)

        # 2. æ·»åŠ å…ƒæ•°æ®
        doc_ids = [str(uuid.uuid4()) for _ in parent_docs]
        for doc, doc_id in zip(parent_docs, doc_ids):
            doc.metadata.update({
                "doc_id": doc_id,
                "source": doc.metadata.get("source", "unknown")
            })

        # 3. å­˜å‚¨åˆ° docstore (ä¿®å¤: ä½¿ç”¨æ­£ç¡®çš„ mset æ ¼å¼)
        docstore_pairs: List[Tuple[str, Document]] = [
            (doc.metadata["doc_id"], doc) for doc in parent_docs
        ]
        st.session_state["docstore"].mset(docstore_pairs)

        # 4. åˆ›å»ºå­æ–‡æ¡£ç”¨äºå‘é‡æ£€ç´¢
        child_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)
        child_docs = child_splitter.split_documents(parent_docs)

        # 5. å­æ–‡æ¡£ä¿ç•™çˆ¶æ–‡æ¡£å¼•ç”¨
        for child_doc in child_docs:
            child_doc.metadata["parent_id"] = child_doc.metadata.get("doc_id", "")
        # 6. æ„å»ºå‘é‡ç´¢å¼•
        vectorstore = get_vectorstore()
        vectorstore.add_documents(child_docs)
        # vectorstore.persist()  # å·²åˆ é™¤ - è‡ªåŠ¨æŒä¹…åŒ–

        # 7. æ›´æ–° BM25
        current_bm25_docs = st.session_state.get("bm25_docs", []) + parent_docs
        rebuild_bm25(current_bm25_docs)

        st.success(
            f"âœ… ç´¢å¼•å®Œæˆï¼\nğŸ“„ çˆ¶æ–‡æ¡£: {len(parent_docs)}\nğŸ” å­æ–‡æ¡£: {len(child_docs)}\nğŸ’¾ å‘é‡åº“: {vectorstore._collection.count()}")

        # é‡ç½®èŠå¤©å†å²
        st.session_state["messages"] = []
        st.session_state["chat_history"] = []


# ============ LLM é…ç½® ============
def get_llm_model(provider: str, model_name: str, temp: float = 0.1):
    load_dotenv()

    api_key = None
    base_url = None

    if provider == "OpenAI":
        api_key = os.getenv("OPENAI_API_KEY")
    elif provider == "DeepSeek":
        api_key = os.getenv("DEEPSEEK_API_KEY")
        base_url = "https://api.deepseek.com"
    elif provider == "Qwen":
        api_key = os.getenv("DASHSCOPE_API_KEY")
        base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"

    if not api_key:
        return None

    return ChatOpenAI(
        model=model_name,
        temperature=temp,
        api_key=api_key.strip(),
        base_url=base_url,
        streaming=True
    )


def rewrite_query(original_query: str, history: List, llm: ChatOpenAI) -> str:
    """æŸ¥è¯¢é‡å†™"""
    if len(history) < 2:
        return original_query

    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªæœç´¢ä¼˜åŒ–ä¸“å®¶ã€‚æ ¹æ®å¯¹è¯å†å²å’Œç”¨æˆ·æœ€æ–°é—®é¢˜ï¼Œ"
        "é‡å†™ä¸ºä¸€ä¸ªç‹¬ç«‹ã€å®Œæ•´ã€åŒ…å«å¿…è¦ä¸Šä¸‹æ–‡çš„æœç´¢æŸ¥è¯¢ã€‚"
        "åªè¾“å‡ºæŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œä¸è¦æ·»åŠ è§£é‡Šã€‚"
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{question}")
    ])

    chain = prompt | llm | StrOutputParser()
    try:
        return chain.invoke({
            "history": history[-6:],
            "question": original_query
        })
    except:
        return original_query


def build_rag_chain(llm):
    """RAG é“¾"""
    qa_system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·ç»“åˆã€å¯¹è¯å†å²ã€‘å’Œã€å‚è€ƒèµ„æ–™ã€‘å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n"
        "è§„åˆ™ï¼š\n"
        "1. ä¼˜å…ˆä¾æ®ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”\n"
        "2. ç»“åˆã€å¯¹è¯å†å²ã€‘ä¿æŒä¸Šä¸‹æ–‡è¿è´¯\n"
        "3. æ— æ³•å›ç­”æ—¶è¯šå®è¯´ä¸çŸ¥é“\n\n"
        "ã€å‚è€ƒèµ„æ–™ã€‘:\n{context}"
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system", qa_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{question}")
    ])

    return prompt | llm | StrOutputParser()


# ============ ä¸»ç¨‹åº ============
def main():
    init_page()

    with st.sidebar:
        st.header("âš™ï¸ é…ç½®")
        provider = st.selectbox("æ¨¡å‹å‚å•†", ["DeepSeek", "OpenAI", "Qwen"])
        default_models = {"DeepSeek": "deepseek-chat", "OpenAI": "gpt-4o-mini", "Qwen": "qwen-turbo"}
        model_name = st.text_input("æ¨¡å‹åç§°", value=default_models[provider])

        st.divider()
        uploaded_files = st.file_uploader(
            "ğŸ“‚ ä¸Šä¼ çŸ¥è¯†åº“",
            accept_multiple_files=True,
            type=['pdf', 'txt', 'md', 'docx']
        )

        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸš€ å¼€å§‹ç´¢å¼•", type="primary", use_container_width=True):
                ingest_files(uploaded_files)
        with col2:
            if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ•°æ®", type="secondary", use_container_width=True):
                # æ¸…ç©ºæ‰€æœ‰çŠ¶æ€
                st.session_state["docstore"] = InMemoryStore()
                st.session_state["bm25"] = None
                st.session_state["bm25_docs"] = []
                st.session_state["messages"] = []
                st.session_state["chat_history"] = []
                try:
                    get_vectorstore().reset_collection()
                    st.success("âœ… å·²æ¸…ç©ºæ‰€æœ‰æ•°æ®")
                except:
                    pass
                st.rerun()

        st.info(f"ğŸ“Š çŸ¥è¯†åº“æ–‡æ¡£: {len(st.session_state.get('bm25_docs', []))}")

    # èŠå¤©å†å²æ˜¾ç¤º
    for msg in st.session_state["messages"]:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
            if msg.get("rewrite"):
                st.caption(f"ğŸ” ä¼˜åŒ–æŸ¥è¯¢: {msg['rewrite']}")
            if msg.get("sources"):
                with st.expander(f"ğŸ“š å‚è€ƒèµ„æ–™ ({len(msg['sources'])} ä»½)"):
                    for i, doc in enumerate(msg["sources"]):
                        st.markdown(f"**[{i + 1}] {doc.metadata.get('source', 'æœªçŸ¥')}**")
                        st.caption(f"ID: {doc.metadata.get('doc_id', 'N/A')}")
                        preview = doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content
                        st.text(preview)

    # ç”¨æˆ·è¾“å…¥å¤„ç†
    if user_input := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
        llm = get_llm_model(provider, model_name)
        if not llm:
            st.error("âŒ è¯·é…ç½®æ­£ç¡®çš„ API Key (.env æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡)")
            st.stop()

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        st.session_state["messages"].append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        # AI å“åº”
        with st.chat_message("assistant"):
            status_container = st.status("ğŸ¤” æ­£åœ¨æ€è€ƒ...", expanded=True)
            final_response = ""
            source_documents = []
            rewritten_query = user_input

            try:
                # 1. æŸ¥è¯¢ä¼˜åŒ–
                status_container.write("ğŸ”„ ç†è§£ä¸Šä¸‹æ–‡...")
                rewritten_query = rewrite_query(
                    user_input,
                    st.session_state["chat_history"],
                    llm
                )
                if rewritten_query != user_input:
                    status_container.write(f"ğŸ” ä¼˜åŒ–æŸ¥è¯¢: `{rewritten_query}`")

                # 2. æ··åˆæ£€ç´¢
                status_container.write("ğŸ“¥ æ‰§è¡Œæ··åˆæ£€ç´¢...")
                hybrid_retriever = get_hybrid_retriever()
                if not hybrid_retriever:
                    st.error("âŒ è¯·å…ˆä¸Šä¼ å¹¶ç´¢å¼•æ–‡æ¡£ï¼")
                    return

                # 3. FlashRank é‡æ’åº (ç®€åŒ–ç‰ˆ)
                status_container.write("âš–ï¸ FlashRank æ™ºèƒ½æ’åº...")
                raw_docs = hybrid_retriever.invoke(rewritten_query)

                # ä½¿ç”¨ FlashRank å‹ç¼©å™¨
                try:
                    reranker = FlashrankRerank(top_n=4)
                    source_documents = reranker.compress_documents(
                        raw_docs, [Document(page_content=rewritten_query)]
                    )
                except:
                    # é™çº§åˆ°ç®€å• Top-K
                    source_documents = raw_docs[:4]

                status_container.write(f"âœ… æ£€ç´¢åˆ° {len(source_documents)} ä»½é«˜è´¨é‡èµ„æ–™")

                # 4. ç”Ÿæˆå›ç­”
                status_container.write("ğŸ’­ ç”Ÿæˆæ™ºèƒ½å›ç­”...")
                context_text = "\n\n".join([d.page_content for d in source_documents])
                chain = build_rag_chain(llm)

                input_dict = {
                    "context": context_text,
                    "question": user_input,
                    "chat_history": st.session_state["chat_history"][-6:]
                }

                placeholder = st.empty()
                for chunk in chain.stream(input_dict):
                    final_response += chunk
                    placeholder.markdown(final_response + "â–Œ")
                placeholder.markdown(final_response)

                status_container.update(label="âœ… å®Œæˆï¼", state="complete")

            except Exception as e:
                st.error(f"âŒ å¤„ç†å‡ºé”™: {str(e)}")
                status_container.update(label="âŒ é”™è¯¯", state="error")
                import traceback
                st.code(traceback.format_exc(), language="python")
                return

        # æ›´æ–°ä¼šè¯çŠ¶æ€
        st.session_state["messages"].append({
            "role": "assistant",
            "content": final_response,
            "rewrite": rewritten_query if rewritten_query != user_input else None,
            "sources": source_documents
        })

        st.session_state["chat_history"].extend([
            HumanMessage(content=user_input),
            AIMessage(content=final_response)
        ])

        # æ§åˆ¶å†å²é•¿åº¦
        if len(st.session_state["chat_history"]) > MAX_HISTORY_LENGTH * 2:
            st.session_state["chat_history"] = st.session_state["chat_history"][-MAX_HISTORY_LENGTH * 2:]


if __name__ == "__main__":
    main()
