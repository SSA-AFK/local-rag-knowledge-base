import os
import tempfile
import uuid
import re
import numpy as np
from typing import List, Any, Tuple
import streamlit as st
from dotenv import load_dotenv

# v1.0 æ ¸å¿ƒå¯¼å…¥
from langchain_core.documents import Document
from langchain_core.stores import InMemoryStore
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.retrievers import BaseRetriever
from langchain_core.messages import HumanMessage, AIMessage

# v1.0 ç»„ä»¶
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.document_compressors import FlashrankRerank


# ä¼˜åŒ–çš„æ··åˆæ£€ç´¢å™¨
class HybridParentRetriever(BaseRetriever):
    """v1.0 æ··åˆæ£€ç´¢å™¨ï¼šå‘é‡æ£€ç´¢(å­æ–‡æ¡£) + BM25(çˆ¶æ–‡æ¡£) + çˆ¶æ–‡æ¡£æ˜ å°„"""

    vectorstore: Chroma
    docstore: InMemoryStore
    bm25_docs: List[Document]
    k1: int = 6  # å‘é‡æ£€ç´¢å­æ–‡æ¡£æ•°é‡
    k2: int = 4  # BM25 æ£€ç´¢çˆ¶æ–‡æ¡£æ•°é‡

    def _get_relevant_documents(self, query: str, *, run_manager: Any = None) -> List[Document]:
        return self.invoke(query, config=run_manager.config if run_manager else None)

    def invoke(self, query: str, config: Any = None, **kwargs: Any) -> List[Document]:
        # 1. å‘é‡æ£€ç´¢å­æ–‡æ¡£
        child_retriever = self.vectorstore.as_retriever(search_kwargs={"k": self.k1})
        child_docs = child_retriever.invoke(query, config=config)

        # 2. BM25 æ£€ç´¢çˆ¶æ–‡æ¡£
        kw_docs = bm25_search_docs(query, self.bm25_docs, top_k=self.k2)

        # 3. âœ… ä¿®å¤ï¼šæ­£ç¡®å¤„ç† mget è¿”å›å€¼
        parent_docs_from_vector = []
        for child_doc in child_docs:
            parent_id = child_doc.metadata.get("parent_id")
            if parent_id:
                # mget è¿”å› List[Optional[Document]]ï¼Œå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆå€¼
                parent_docs = self.docstore.mget([parent_id])
                if parent_docs and len(parent_docs) > 0 and parent_docs[0] is not None:
                    parent_docs_from_vector.append(parent_docs[0])

        # 4. å»é‡é€»è¾‘ä¿æŒä¸å˜
        all_docs = parent_docs_from_vector + kw_docs
        seen_doc_ids = set()
        unique_parent_docs = []

        for doc in all_docs:
            doc_id = doc.metadata.get("doc_id")
            if doc_id and doc_id not in seen_doc_ids:
                seen_doc_ids.add(doc_id)
                unique_parent_docs.append(doc)

        return unique_parent_docs[:self.k1 + self.k2]



# ============ å…¨å±€é…ç½® ============
CHROMA_PERSIST_DIRECTORY = "./chroma_db_parent"
EMB_MODEL_NAME = "BAAI/bge-small-zh-v1.5"
MAX_HISTORY_LENGTH = 10


# ============ åˆå§‹åŒ–é¡µé¢ ============
def init_page() -> None:
    st.set_page_config(page_title="Pro RAG (LangChain v1.0)", page_icon="ğŸš€", layout="wide")

    st.markdown("""
        <style>
        .main-header {font-size: 1.8rem; font-weight: 700; color: #1f77b4;}
        .stChatInput {position: fixed; bottom: 20px;}
        </style>
    """, unsafe_allow_html=True)

    st.markdown('<div class="main-header">ğŸš€ Pro RAG v1.0: Hybrid Search + FlashRank</div>',
                unsafe_allow_html=True)

    # åˆå§‹åŒ– Session State
    if "docstore" not in st.session_state:
        st.session_state["docstore"] = InMemoryStore()
    if "bm25" not in st.session_state:
        st.session_state["bm25"] = None
    if "bm25_docs" not in st.session_state:
        st.session_state["bm25_docs"] = []
    if "messages" not in st.session_state:
        st.session_state["messages"] = []
    if "chat_history" not in st.session_state:
        st.session_state["chat_history"] = []


# ============ æ ¸å¿ƒç»„ä»¶ ============
@st.cache_resource
def get_embeddings() -> HuggingFaceEmbeddings:
    os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"
    return HuggingFaceEmbeddings(
        model_name=EMB_MODEL_NAME,
        encode_kwargs={"normalize_embeddings": True},
        model_kwargs={"device": "cpu"}
    )


def get_vectorstore() -> Chroma:
    embedding = get_embeddings()

    vectorstore = Chroma(
        collection_name="split_parents",
        persist_directory=CHROMA_PERSIST_DIRECTORY,
        embedding_function=embedding
    )

    # å¦‚æœ docstore ä¸ºç©ºä½† chroma æœ‰æ•°æ®ï¼Œé‡ç½®
    if len(list(st.session_state["docstore"].yield_keys())) == 0 and vectorstore._collection.count() > 0:
        st.warning("âš ï¸ DocStore ä¸ Chroma ä¸ä¸€è‡´ï¼Œæ­£åœ¨é‡ç½®...")
        vectorstore.reset_collection()

    return vectorstore


def get_hybrid_retriever() -> HybridParentRetriever:
    """v1.0 æ··åˆæ£€ç´¢å™¨æ„å»º"""
    if not st.session_state["bm25_docs"]:
        return None

    return HybridParentRetriever(
        vectorstore=get_vectorstore(),
        docstore=st.session_state["docstore"],
        bm25_docs=st.session_state["bm25_docs"]
    )


# ============ BM25 å®ç° ============
def _tokenize_zh(text: str) -> List[str]:
    return re.findall(r"[\u4e00-\u9fff]+|[a-zA-Z0-9]+", text)


def rebuild_bm25(docs: List[Document]):
    """é‡å»º BM25 ç´¢å¼•"""
    if not docs:
        st.session_state["bm25"] = None
        return

    try:
        from rank_bm25 import BM25Okapi
        tokenized_corpus = [_tokenize_zh(d.page_content) for d in docs]
        st.session_state["bm25"] = BM25Okapi(tokenized_corpus)
        st.session_state["bm25_docs"] = docs
    except ImportError:
        st.error("è¯·å®‰è£…: pip install rank_bm25")
        st.stop()


def bm25_search_docs(query: str, docs: List[Document], top_k: int = 4) -> List[Document]:
    """BM25 æ£€ç´¢"""
    bm25 = st.session_state.get("bm25")
    if not bm25 or not docs:
        return docs[:top_k]  # é™çº§åˆ°ç®€å•åˆ‡ç‰‡

    tokenized_query = _tokenize_zh(query)
    scores = bm25.get_scores(tokenized_query)
    top_indices = np.argsort(scores)[-top_k:][::-1]

    results = [docs[i] for i in top_indices if scores[i] > 0]
    return results[:top_k]


# ============ æ–‡æ¡£å¤„ç† ============
def ingest_files(uploaded_files):
    """æ–‡æ¡£ç´¢å¼• - v1.0 ä¿®å¤ç‰ˆ"""
    if not uploaded_files:
        return

    raw_docs = []
    with st.status("ğŸ“¥ æ­£åœ¨å¤„ç†æ–‡æ¡£...", expanded=True) as status:
        for file in uploaded_files:
            status.write(f"è§£ææ–‡ä»¶: {file.name}")
            suffix = os.path.splitext(file.name)[1].lower()

            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
                tmp.write(file.getbuffer())
                tmp_path = tmp.name

            try:
                if suffix == ".pdf":
                    loader = PyPDFLoader(tmp_path)
                    raw_docs.extend(loader.load())
                elif suffix in [".txt", ".md"]:
                    loader = TextLoader(tmp_path, encoding="utf-8")
                    raw_docs.extend(loader.load())
                elif suffix in [".docx", ".doc"]:
                    try:
                        import docx2txt
                        text = docx2txt.process(tmp_path)
                        raw_docs.append(Document(
                            page_content=text,
                            metadata={"source": file.name}
                        ))
                    except ImportError:
                        st.warning("docx2txt æœªå®‰è£…ï¼Œè·³è¿‡ DOCX æ–‡ä»¶")
                        continue
            except Exception as e:
                st.error(f"âŒ è§£æå¤±è´¥ {file.name}: {e}")
            finally:
                try:
                    os.remove(tmp_path)
                except:
                    pass

        if not raw_docs:
            st.error("âŒ æœªè§£æåˆ°ä»»ä½•æ–‡æ¡£å†…å®¹")
            return

        status.write("ğŸ”¨ æ„å»ºçŸ¥è¯†åº“ç´¢å¼•...")

        # 1. çˆ¶æ–‡æ¡£åˆ†å‰² (å¤§å—)
        parent_splitter = RecursiveCharacterTextSplitter(
            chunk_size=2000, chunk_overlap=200
        )
        parent_docs = parent_splitter.split_documents(raw_docs)

        # 2. æ·»åŠ å…ƒæ•°æ®åˆ°çˆ¶æ–‡æ¡£
        doc_ids = [str(uuid.uuid4()) for _ in parent_docs]
        parent_doc_map = {}  # æ–°å¢ï¼šçˆ¶æ–‡æ¡£æ˜ å°„è¡¨

        for doc, doc_id in zip(parent_docs, doc_ids):
            doc.metadata.update({
                "doc_id": doc_id,
                "source": doc.metadata.get("source", "unknown")
            })
            parent_doc_map[doc_id] = doc  # å­˜å‚¨çˆ¶æ–‡æ¡£æ˜ å°„

        # 3. å­˜å‚¨çˆ¶æ–‡æ¡£åˆ° docstore
        docstore_pairs = [(doc.metadata["doc_id"], doc) for doc in parent_docs]
        st.session_state["docstore"].mset(docstore_pairs)

        # 4. âœ… ä¿®å¤ï¼šåˆ›å»ºå­æ–‡æ¡£ + æ˜ç¡® parent_id æ˜ å°„
        child_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)
        child_docs = []

        for parent_doc in parent_docs:
            parent_id = parent_doc.metadata["doc_id"]
            parent_content = parent_doc.page_content

            # åˆ†å‰²çˆ¶æ–‡æ¡£å†…å®¹ä¸ºå­æ–‡æ¡£
            child_texts = child_splitter.split_text(parent_content)

            for i, child_text in enumerate(child_texts):
                child_doc = Document(
                    page_content=child_text,
                    metadata={
                        "parent_id": parent_id,  # âœ… æ˜ç¡®å…³è”çˆ¶æ–‡æ¡£
                        "doc_id": f"child_{parent_id}_{i}",
                        "source": parent_doc.metadata["source"],
                        "chunk_index": i,
                        "parent_source": parent_doc.metadata["source"]
                    }
                )
                child_docs.append(child_doc)

        # 5. æ„å»ºå‘é‡ç´¢å¼•
        vectorstore = get_vectorstore()
        vectorstore.add_documents(child_docs)

        # 6. æ›´æ–° BM25ï¼ˆåªç”¨çˆ¶æ–‡æ¡£ï¼‰
        rebuild_bm25(parent_docs)  # åªä¼ å…¥çˆ¶æ–‡æ¡£ï¼Œä¸ç´¯ç§¯

        st.success(f"âœ… ç´¢å¼•å®Œæˆï¼çˆ¶æ–‡æ¡£: {len(parent_docs)}, å­æ–‡æ¡£: {len(child_docs)}")

        # é‡ç½®èŠå¤©å†å²
        st.session_state["messages"] = []
        st.session_state["chat_history"] = []


# ============ LLM é…ç½® ============
def get_llm_model(provider: str, model_name: str, temp: float = 0.1):
    load_dotenv()

    api_key = None
    base_url = None

    if provider == "OpenAI":
        api_key = os.getenv("OPENAI_API_KEY")
    elif provider == "DeepSeek":
        api_key = os.getenv("DEEPSEEK_API_KEY")
        base_url = "https://api.deepseek.com"
    elif provider == "Qwen":
        api_key = os.getenv("DASHSCOPE_API_KEY")
        base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"

    if not api_key:
        return None

    return ChatOpenAI(
        model=model_name,
        temperature=temp,
        api_key=api_key.strip(),
        base_url=base_url,
        streaming=True
    )


def rewrite_query(original_query: str, history: List, llm: ChatOpenAI) -> str:
    """æŸ¥è¯¢é‡å†™"""
    if len(history) < 2:
        return original_query

    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªæœç´¢ä¼˜åŒ–ä¸“å®¶ã€‚æ ¹æ®å¯¹è¯å†å²å’Œç”¨æˆ·æœ€æ–°é—®é¢˜ï¼Œ"
        "é‡å†™ä¸ºä¸€ä¸ªç‹¬ç«‹ã€å®Œæ•´ã€åŒ…å«å¿…è¦ä¸Šä¸‹æ–‡çš„æœç´¢æŸ¥è¯¢ã€‚"
        "åªè¾“å‡ºæŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œä¸è¦æ·»åŠ è§£é‡Šã€‚"
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{question}")
    ])

    chain = prompt | llm | StrOutputParser()
    try:
        return chain.invoke({
            "history": history[-6:],
            "question": original_query
        })
    except:
        return original_query


def build_rag_chain(llm):
    """RAG é“¾"""
    qa_system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·ç»“åˆã€å¯¹è¯å†å²ã€‘å’Œã€å‚è€ƒèµ„æ–™ã€‘å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n"
        "è§„åˆ™ï¼š\n"
        "1. ä¼˜å…ˆä¾æ®ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”\n"
        "2. ç»“åˆã€å¯¹è¯å†å²ã€‘ä¿æŒä¸Šä¸‹æ–‡è¿è´¯\n"
        "3. æ— æ³•å›ç­”æ—¶è¯šå®è¯´ä¸çŸ¥é“\n\n"
        "ã€å‚è€ƒèµ„æ–™ã€‘:\n{context}"
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system", qa_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{question}")
    ])

    return prompt | llm | StrOutputParser()


# ============ ä¸»ç¨‹åº ============
def main():
    init_page()

    with st.sidebar:
        st.header("âš™ï¸ é…ç½®")
        provider = st.selectbox("æ¨¡å‹å‚å•†", ["DeepSeek", "OpenAI", "Qwen"])
        default_models = {"DeepSeek": "deepseek-chat", "OpenAI": "gpt-4o-mini", "Qwen": "qwen-turbo"}
        model_name = st.text_input("æ¨¡å‹åç§°", value=default_models[provider])

        st.divider()
        uploaded_files = st.file_uploader(
            "ğŸ“‚ ä¸Šä¼ çŸ¥è¯†åº“",
            accept_multiple_files=True,
            type=['pdf', 'txt', 'md', 'docx']
        )

        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸš€ å¼€å§‹ç´¢å¼•", type="primary", use_container_width=True):
                ingest_files(uploaded_files)
        with col2:
            if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ•°æ®", type="secondary", use_container_width=True):
                # æ¸…ç©ºæ‰€æœ‰çŠ¶æ€
                st.session_state["docstore"] = InMemoryStore()
                st.session_state["bm25"] = None
                st.session_state["bm25_docs"] = []
                st.session_state["messages"] = []
                st.session_state["chat_history"] = []
                try:
                    get_vectorstore().reset_collection()
                    st.success("âœ… å·²æ¸…ç©ºæ‰€æœ‰æ•°æ®")
                except:
                    pass
                st.rerun()

        st.info(f"ğŸ“Š çŸ¥è¯†åº“æ–‡æ¡£: {len(st.session_state.get('bm25_docs', []))}")

    # èŠå¤©å†å²æ˜¾ç¤º
    for msg_idx, msg in enumerate(st.session_state["messages"]):
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
            if msg.get("rewrite"):
                st.caption(f"ğŸ” ä¼˜åŒ–æŸ¥è¯¢: {msg['rewrite']}")

            # âœ… ä¿®å¤ï¼šåœ¨èŠå¤©å†å²æ˜¾ç¤ºéƒ¨åˆ†
            if msg.get("sources"):
                with st.expander(f"ğŸ“š å‚è€ƒèµ„æ–™ ({len(msg['sources'])} ä»½)"):
                    for i, doc in enumerate(msg["sources"]):
                        st.markdown(f"**[{i + 1}] {doc.metadata.get('source', 'æœªçŸ¥')}**")

                        doc_id = doc.metadata.get('doc_id', 'N/A')
                        parent_id = doc.metadata.get('parent_id', 'N/A')
                        st.caption(f"ID: {doc_id} | çˆ¶æ–‡æ¡£: {parent_id}")

                        preview = doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content
                        # âœ… ä¿®å¤ï¼šä½¿ç”¨æ¶ˆæ¯ç´¢å¼•+æ–‡æ¡£ç´¢å¼•ä½œä¸ºç¨³å®š key
                        st.text_area(
                            f"preview_history_{msg_idx}_{i}",
                            preview,
                            height=100
                        )

    # ç”¨æˆ·è¾“å…¥å¤„ç†
    if user_input := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
        llm = get_llm_model(provider, model_name)
        if not llm:
            st.error("âŒ è¯·é…ç½®æ­£ç¡®çš„ API Key (.env æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡)")
            st.stop()

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        st.session_state["messages"].append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        # AI å“åº”
        # AI å“åº”éƒ¨åˆ† - ä¿®å¤ç‰ˆ
        with st.chat_message("assistant"):
            status_container = st.status("ğŸ¤” æ­£åœ¨æ€è€ƒ...")
            final_response = ""


            try:
                # 1. æŸ¥è¯¢ä¼˜åŒ–
                status_container.write("ğŸ”„ ç†è§£ä¸Šä¸‹æ–‡...")
                rewritten_query = rewrite_query(
                    user_input,
                    st.session_state["chat_history"],
                    llm
                )
                if rewritten_query != user_input:
                    status_container.write(f"ğŸ” ä¼˜åŒ–æŸ¥è¯¢: `{rewritten_query}`")

                # 2. æ··åˆæ£€ç´¢ - å…ˆå®Œæ•´æ‰§è¡Œï¼Œå†æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
                status_container.write("ğŸ“¥ æ‰§è¡Œæ··åˆæ£€ç´¢...")
                hybrid_retriever = get_hybrid_retriever()
                if not hybrid_retriever:
                    st.error("âŒ è¯·å…ˆä¸Šä¼ å¹¶ç´¢å¼•æ–‡æ¡£ï¼")
                    return

                # æ‰§è¡Œå®Œæ•´æ£€ç´¢
                raw_docs = hybrid_retriever.invoke(rewritten_query)

                # 3. FlashRank é‡æ’åº
                status_container.write("âš–ï¸ FlashRank æ™ºèƒ½æ’åº...")
                try:
                    reranker = FlashrankRerank(top_n=4)
                    source_documents = reranker.compress_documents(
                        raw_docs, [Document(page_content=rewritten_query)]
                    )
                except:
                    source_documents = raw_docs[:4]

                # âœ… ç°åœ¨ source_documents å·²æ­£ç¡®èµ‹å€¼ï¼Œå†æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
                status_container.write(f"âœ… æ£€ç´¢åˆ° {len(source_documents)} ä»½é«˜è´¨é‡èµ„æ–™")
                status_container.write("ğŸ” æ£€ç´¢è¯¦æƒ…:")

                # è·å–è°ƒè¯•ç”¨çš„å­æ–‡æ¡£å’ŒBM25æ–‡æ¡£ï¼ˆç”¨äºæ˜¾ç¤ºï¼Œä¸å½±å“source_documentsï¼‰
                child_retriever = hybrid_retriever.vectorstore.as_retriever(
                    search_kwargs={"k": hybrid_retriever.k1}
                )
                child_docs = child_retriever.invoke(rewritten_query)
                kw_docs = bm25_search_docs(
                    rewritten_query,
                    hybrid_retriever.bm25_docs,
                    top_k=hybrid_retriever.k2
                )

                status_container.write(f"  - å‘é‡å­æ–‡æ¡£: {len(child_docs)}")
                status_container.write(f"  - BM25çˆ¶æ–‡æ¡£: {len(kw_docs)}")
                status_container.write(f"  - FlashRankå: {len(source_documents)}")
                status_container.write(
                    f"  - ç¤ºä¾‹æ–‡æ¡£: {[d.metadata.get('source', 'N/A')[:30] for d in source_documents[:2]]}")

                # 4. ç”Ÿæˆå›ç­”
                status_container.write("ğŸ’­ ç”Ÿæˆæ™ºèƒ½å›ç­”...")
                context_text = "\n\n".join([d.page_content for d in source_documents])
                chain = build_rag_chain(llm)

                input_dict = {
                    "context": context_text,
                    "question": user_input,
                    "chat_history": st.session_state["chat_history"][-6:]
                }

                placeholder = st.empty()
                for chunk in chain.stream(input_dict):
                    final_response += chunk
                    placeholder.markdown(final_response + "â–Œ")
                placeholder.markdown(final_response)

                status_container.update(label="âœ… å®Œæˆï¼", state="complete")

                # ğŸ”¥ æ–°å¢ï¼šç«‹å³æ˜¾ç¤ºå½“å‰æ¶ˆæ¯çš„å‚è€ƒèµ„æ–™
                if source_documents:
                    # âŒ åˆ é™¤ expanded=True å‚æ•°
                    with st.expander(f"ğŸ“š å‚è€ƒèµ„æ–™ ({len(source_documents)} ä»½)"):  # âœ… å·²ä¿®å¤
                        for i, doc in enumerate(source_documents):
                            st.markdown(f"**[{i + 1}] {doc.metadata.get('source', 'æœªçŸ¥')}**")

                            doc_id = doc.metadata.get('doc_id', 'N/A')
                            parent_id = doc.metadata.get('parent_id', 'N/A')
                            st.caption(f"ID: {doc_id} | çˆ¶æ–‡æ¡£: {parent_id}")

                            preview = doc.page_content[:500] + "..." if len(
                                doc.page_content) > 500 else doc.page_content
                            # âœ… åŒæ—¶ä¿®å¤ key å†²çª
                            current_msg_idx = len(st.session_state["messages"])
                            st.text_area(
                                f"preview_current_{current_msg_idx}_{i}",  # âœ… ç¨³å®š key
                                preview,
                                height=100
                            )

            except Exception as e:
                # ... é”™è¯¯å¤„ç†ä¿æŒä¸å˜ ...
                return

            # ç„¶åæ‰æ›´æ–°ä¼šè¯çŠ¶æ€ï¼ˆç°æœ‰ä»£ç ï¼‰
            st.session_state["messages"].append({
                "role": "assistant",
                "content": final_response,
                "rewrite": rewritten_query if rewritten_query != user_input else None,
                "sources": source_documents  # è¿™ä¸ªç”¨äºå†å²æ˜¾ç¤º
            })

        st.session_state["chat_history"].extend([
            HumanMessage(content=user_input),
            AIMessage(content=final_response)
        ])

        # æ§åˆ¶å†å²é•¿åº¦
        if len(st.session_state["chat_history"]) > MAX_HISTORY_LENGTH * 2:
            st.session_state["chat_history"] = st.session_state["chat_history"][-MAX_HISTORY_LENGTH * 2:]




if __name__ == "__main__":
    main()
